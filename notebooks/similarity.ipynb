{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Word Vectors\n",
    "\n",
    "Welcome to your first assignment of Week 2, Course 5 of the Deep Learning Specialization! \n",
    "\n",
    "Because word embeddings are very computationally expensive to train, most ML practitioners will load a pre-trained set of embeddings. In this notebook you'll try your hand at loading, measuring similarity between, and modifying pre-trained embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.similarity_utils.w2v_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Vectors Versus One-Hot Vectors\n",
    "One-hot vectors don't do a good job of capturing the level of similarity between words. This is because every one-hot vector has the same Euclidean distance from any other one-hot vector.\n",
    "\n",
    "Embedding vectors, such as GloVe vectors, provide much more useful information about the meaning of individual words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('../datasets/texts/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
    "\n",
    "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
    "* $||u||_2$ is the norm (or length) of the vector $u$\n",
    "* $\\theta$ is the angle between $u$ and $v$. \n",
    "* The cosine similarity depends on the angle between $u$ and $v$. \n",
    "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
    "    * If they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "<center><img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\"></center>\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Special case. Consider the case u = [0, 0], v=[0, 0]\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "    \n",
    "    # Compute the dot product between u and v\n",
    "    dot = np.dot(u, v) \n",
    "    norm_u = np.sqrt(np.sum(np.power(u, 2)))\n",
    "    \n",
    "    norm_v = np.linalg.norm(v)\n",
    "    \n",
    "    # Avoid division by 0\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "    \n",
    "    # Compute the cosine similarity defined by formula (1) \n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy Task\n",
    "\n",
    "* In the word analogy task, complete this sentence:  \n",
    "    <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. \n",
    "\n",
    "* An example is:  \n",
    "    <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n",
    "\n",
    "* You're trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n",
    "    $e_b - e_a \\approx e_d - e_c$\n",
    "* Measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n",
    "\n",
    "<a name='ex-2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b and e_c\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "    \n",
    "    # loop over the whole word vector set\n",
    "    for w in words:   \n",
    "        # to avoid best_word being one the input words, skip the input word_c\n",
    "        # skip word_c from query\n",
    "        if w == word_c:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w \n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> smaller\n",
      "small -> smaller :: big -> competitors\n",
      "weak -> weakness :: strong -> weakness\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'),\n",
    "                 ('india', 'delhi', 'japan'),\n",
    "                 ('man', 'woman', 'boy'),\n",
    "                 ('small', 'smaller', 'large')]\n",
    "\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion*\n",
    "\n",
    "- Cosine similarity is a good way to compare the similarity between pairs of word vectors.\n",
    "    - Note that L2 (Euclidean) distance also works.\n",
    "- For NLP applications, using a pre-trained set of word vectors is often a great way to get started. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
