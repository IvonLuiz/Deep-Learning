{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using Numpy to build neural networks, now we will explore a deep learning framework that facilitates the implementation of DL algorithms. Machine learning frameworks like TensorFlow, PaddlePaddle, Torch, Caffe, Keras, and many others can speed up your machine learning development significantly. TensorFlow 2.3 has made significant improvements over its predecessor, some of which we'll encounter and implement here!\n",
    "\n",
    "This frameworks not only cut down on time spent coding, but can also perform optimizations that speed up the code itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Optimization with GradientTape\n",
    "\n",
    "The beauty of TensorFlow 2 is in its simplicity. Basically, all that is needed to do is implement forward propagation through a computational graph. TensorFlow will compute the derivatives for you, by moving backwards through the graph recorded with `GradientTape`. All that's left to do then is specify the cost function and optimizer to use. \n",
    "\n",
    "When writing a TensorFlow program, the main object to get used and transformed is the `tf.Tensor`. These tensors are the TensorFlow equivalent of Numpy arrays, i.e. multidimensional arrays of a given data type that also contain information about the computational graph.\n",
    "\n",
    "Below, we use `tf.Variable` to store the state of your variables. Variables can only be created once as its initial value defines the variable shape and type. Additionally, the `dtype` arg in `tf.Variable` can be set to allow data to be converted to that type. But if none is specified, either the datatype will be kept if the initial value is a Tensor, or `convert_to_tensor` will decide. It's generally best to specify directly, so nothing breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(W, X, b):\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- Y = WX + b \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "#     X = tf.constant(np.random.randn(3, 1), name = \"X\")\n",
    "#     W = tf.constant(np.random.randn(4, 3), name = \"W\")\n",
    "#     b = tf.constant(np.random.randn(4, 1), name = \"X\")\n",
    "    \n",
    "    Y = tf.add(tf.matmul(W, X), b)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    a -- (tf.float32) the sigmoid of z\n",
    "    \"\"\"\n",
    "\n",
    "    z = tf.cast(z , tf.float32)\n",
    "    a = tf.keras.activations.sigmoid(z)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, C=6):\n",
    "    \"\"\"\n",
    "    Computes the one hot encoding for a single label\n",
    "\n",
    "    Arguments:\n",
    "        label --  (int) Categorical labels\n",
    "        C --  (int) Number of different classes that label can take\n",
    "    \n",
    "    Returns:\n",
    "         one_hot -- tf.Tensor A one-dimensional tensor (array) with the one hot encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot = tf.reshape(tf.one_hot(label, depth=C), [C])\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes in a shape and returns an array of numbers using the GlorotNormal initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with TensorFlow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "                                \n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=1)   \n",
    "\n",
    "    W1 = tf.Variable(initializer(shape=([25, 12288])))\n",
    "    b1 = tf.Variable(initializer(shape=([25, 1])))\n",
    "    W2 = tf.Variable(initializer(shape=([12, 25])))\n",
    "    b2 = tf.Variable(initializer(shape=([12, 1])))\n",
    "    W3 = tf.Variable(initializer(shape=([6, 12])))\n",
    "    b3 = tf.Variable(initializer(shape=([6, 1])))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "                                            # Numpy Equivalents (NumPy not to be used. Use TF API):\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)       # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.keras.activations.relu(Z1)      # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)      # Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = tf.keras.activations.relu(Z2)      # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)      # Z3 = np.dot(W3, A2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Total Loss\n",
    "\n",
    "For our case, since we have a multi classification problem, a categorical cross entropy will be used.\n",
    "\n",
    "Before we used to compute the cost value which sums the losses over the whole batch (i.e. all mini-batches) of samples, then divide the sum by the total number of samples. Here, you will achieve this in two steps. \n",
    "\n",
    "In step 1, the `compute_total_loss` function will only take care of summing the losses from one mini-batch of samples. Then, as you train the model by calling the `compute_total_loss` function once per mini-batch, step 2 will be done by accumulating the sums from each of the mini-batches, and finishing it with the division by the total number of samples to get the final cost value.\n",
    "\n",
    "Computing the \"total loss\" instead of \"mean loss\" in step 1 can make sure the final cost value to be consistent. For example, if the mini-batch size is 4 but there are just 5 samples in the whole batch, then the last mini-batch is going to have 1 sample only. Considering the 5 samples, losses to be [0, 1, 2, 3, 4] respectively, we know the final cost should be their average which is 2. Adopting the \"total loss\" approach will get us the same answer. However, the \"mean loss\" approach will first get us 1.5 and 4 for the two mini-batches, and then finally 2.75 after taking average of them, which is different from the desired result of 2. Therefore, the \"total loss\" approach is adopted here. \n",
    "\n",
    "To compute the loss: \n",
    "- The \"`y_pred`\" and \"`y_true`\" inputs of [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) are expected to be of shape (number of examples, num_classes). \n",
    "\n",
    "- `tf.reduce_sum` does the summation over the examples.\n",
    "\n",
    "- We skipped applying \"softmax\" in `forwad_propagation` because now it will be taken care by the `tf.keras.losses.categorical_crossentropy` by setting its parameter `from_logits=True` (Explanation by one the DeepLearning.AI mentors [here](https://community.deeplearning.ai/t/week-3-assignment-compute-total-loss-try-to-set-from-logits-false/243049/2?u=paulinpaloalto) in the Community for the mathematical reasoning behind it. If you are not part of the Community already, you can do so by going [here](https://www.coursera.org/learn/deep-neural-network/ungradedLti/ZE1VR/important-have-questions-issues-or-ideas-join-our-community).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the total loss\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- output of forward propagation (output of the last LINEAR unit), of shape (6, num_examples)\n",
    "    labels -- \"true\" labels vector, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    total_loss - Tensor of the total loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.keras.losses.categorical_crossentropy(tf.transpose(labels),\n",
    "                                                    tf.transpose(logits),\n",
    "                                                    from_logits=True)\n",
    "    total_loss = tf.reduce_sum(loss)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When using sum of losses for gradient computation, it’s important to reduce the learning rate as the size of the mini-batch increases. This ensures that we don’t take large steps towards minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "From the optimizers available, we need to specify the type of optimizer in one line, in this case `tf.keras.optimizers.Adam` (though you can use others such as SGD), and then call it within the training loop. \n",
    "\n",
    "Notice the `tape.gradient` function: this allows to retrieve the operations recorded for automatic differentiation inside the `GradientTape` block. Then, calling the optimizer method `apply_gradients`, will apply the optimizer's update rules to each trainable parameter. For more information, review the [documentation](https://www.tensorflow.org/api_docs/python/tf/GradientTape).\n",
    "\n",
    "\n",
    "One extra step that's been added to the batch training process is: \n",
    "\n",
    "- `tf.Data.dataset = dataset.prefetch(8)` \n",
    "\n",
    "What this does is prevent a memory bottleneck that can occur when reading from disk. `prefetch()` sets aside some data and keeps it ready for when it's needed. It does this by creating a source dataset from the input data, applying a transformation to preprocess the data, then iterating over the dataset the specified number of elements at a time. This works because the iteration is streaming, so the data doesn't need to fit into the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 10 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialize your parameters\n",
    "    #(1 line)\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # The CategoricalAccuracy will track the accuracy for this multiclass problem\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
    "    \n",
    "    # We can get the number of elements of a dataset using the cardinality method\n",
    "    m = dataset.cardinality().numpy()\n",
    "    \n",
    "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
    "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
    "    #X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)# <<< extra step    \n",
    "    #Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster \n",
    "\n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_total_loss = 0.\n",
    "        \n",
    "        #We need to reset object to start measuring from 0 the accuracy each epoch\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        for (minibatch_X, minibatch_Y) in minibatches:\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # 1. predict\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "\n",
    "                # 2. loss\n",
    "                minibatch_total_loss = compute_total_loss(Z3, tf.transpose(minibatch_Y))\n",
    "\n",
    "            # We accumulate the accuracy of all the batches\n",
    "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            \n",
    "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "            grads = tape.gradient(minibatch_total_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "            epoch_total_loss += minibatch_total_loss\n",
    "        \n",
    "        # We divide the epoch total loss over the number of samples\n",
    "        epoch_total_loss /= m\n",
    "\n",
    "        # Print the cost every 10 epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_total_loss))\n",
    "            print(\"Train accuracy:\", train_accuracy.result())\n",
    "            \n",
    "            # We evaluate the test set every 10 epochs to avoid computational overhead\n",
    "            for (minibatch_X, minibatch_Y) in test_minibatches:\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            print(\"Test_accuracy:\", test_accuracy.result())\n",
    "\n",
    "            costs.append(epoch_total_loss)\n",
    "            train_acc.append(train_accuracy.result())\n",
    "            test_acc.append(test_accuracy.result())\n",
    "            test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_acc, test_acc = model(new_train,\n",
    "                                               new_y_train,\n",
    "                                               new_test,\n",
    "                                               new_y_test,\n",
    "                                               num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
